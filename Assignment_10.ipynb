{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Part 1: Load & Inspect Data\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import nltk\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Keep only the top N most frequent words\n",
        "TOP_WORDS = 10000\n",
        "(X_train_raw, y_train_raw), (X_test_raw, y_test_raw) = imdb.load_data(num_words=TOP_WORDS)\n",
        "\n",
        "print(\"Train size:\", len(X_train_raw))\n",
        "print(\"Test size:\", len(X_test_raw))\n",
        "print(\"Labels:\", np.unique(y_train_raw))\n",
        "\n",
        "# Reverse dictionary for decoding\n",
        "word_to_index = imdb.get_word_index()\n",
        "index_to_word = {v + 3: k for k, v in word_to_index.items()}\n",
        "index_to_word[0] = \"<PAD>\"\n",
        "index_to_word[1] = \"<START>\"\n",
        "index_to_word[2] = \"<UNK>\"\n",
        "\n",
        "def decode_review(encoded):\n",
        "    return \" \".join([index_to_word.get(i, \"?\") for i in encoded])\n",
        "\n",
        "# Show an example\n",
        "print(\"\\nSample review:\\n\", decode_review(X_train_raw[0]))\n",
        "print(\"Label:\", y_train_raw[0])\n",
        "\n",
        "\n",
        "# Part 1: Preprocessing Function\n",
        "\n",
        "\n",
        "stop_words_set = set(stopwords.words(\"english\"))\n",
        "\n",
        "def clean_text(text):\n",
        "    # lowercase\n",
        "    text = text.lower()\n",
        "    # remove punctuation\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    # remove stopwords\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words_set]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Decode and clean reviews\n",
        "X_train_text = [clean_text(decode_review(r)) for r in X_train_raw]\n",
        "X_test_text = [clean_text(decode_review(r)) for r in X_test_raw]\n",
        "\n",
        "\n",
        "# Part 2: Feature Extraction\n",
        "\n",
        "\n",
        "# TF-IDF features with unigrams & bigrams\n",
        "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "X_train_features = tfidf.fit_transform(X_train_text)\n",
        "X_test_features = tfidf.transform(X_test_text)\n",
        "\n",
        "print(\"TF-IDF shape (train):\", X_train_features.shape)\n",
        "print(\"TF-IDF shape (test):\", X_test_features.shape)\n",
        "\n",
        "\n",
        "# Part 3: Model Training & Eval\n",
        "\n",
        "\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=200),\n",
        "    \"Naive Bayes\": MultinomialNB(),\n",
        "    \"SVM\": LinearSVC()\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, clf in models.items():\n",
        "    clf.fit(X_train_features, y_train_raw)\n",
        "    preds = clf.predict(X_test_features)\n",
        "    acc = accuracy_score(y_test_raw, preds)\n",
        "    prec = precision_score(y_test_raw, preds)\n",
        "    rec = recall_score(y_test_raw, preds)\n",
        "    f1 = f1_score(y_test_raw, preds)\n",
        "    results[name] = (acc, prec, rec, f1)\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(classification_report(y_test_raw, preds))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_raw, preds))\n",
        "\n",
        "# Show comparison\n",
        "print(\"\\nModel Performance Summary:\")\n",
        "for model_name, metrics in results.items():\n",
        "    print(f\"{model_name}: Acc={metrics[0]:.3f}, Prec={metrics[1]:.3f}, Rec={metrics[2]:.3f}, F1={metrics[3]:.3f}\")\n",
        "\n",
        "\n",
        "# Part 4: Pipeline + Tuning\n",
        "\n",
        "\n",
        "pipe = Pipeline([\n",
        "    (\"vectorizer\", TfidfVectorizer(stop_words=\"english\")),\n",
        "    (\"clf\", LogisticRegression(max_iter=300))\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    \"vectorizer__max_features\": [3000, 5000],\n",
        "    \"vectorizer__ngram_range\": [(1, 1), (1, 2)],\n",
        "    \"clf__C\": [0.5, 1, 2]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(pipe, param_grid, cv=3, scoring=\"accuracy\", n_jobs=-1)\n",
        "grid.fit(X_train_text, y_train_raw)\n",
        "\n",
        "print(\"\\nBest Pipeline Parameters:\", grid.best_params_)\n",
        "print(\"Best CV Accuracy:\", grid.best_score_)\n",
        "\n",
        "\n",
        "# Part 5: Inference\n",
        "\n",
        "\n",
        "final_model = grid.best_estimator_\n",
        "\n",
        "test_examples = X_test_text[:5]\n",
        "pred_labels = final_model.predict(test_examples)\n",
        "\n",
        "for i, review in enumerate(test_examples):\n",
        "    print(f\"\\nReview {i+1}: {review[:150]}...\")\n",
        "    print(\"Predicted:\", \"Positive\" if pred_labels[i] == 1 else \"Negative\")\n",
        "    print(\"Actual:\", \"Positive\" if y_test_raw[i] == 1 else \"Negative\")\n",
        "\n",
        "# Final accuracy on test set\n",
        "final_acc = accuracy_score(y_test_raw, final_model.predict(X_test_text))\n",
        "print(\"\\nFinal Test Accuracy:\", final_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nn5rdmKYRbjc",
        "outputId": "75fe0e0e-d801-4e34-e06a-b250ec3f3977"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 25000\n",
            "Test size: 25000\n",
            "Labels: [0 1]\n",
            "\n",
            "Sample review:\n",
            " <START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
            "Label: 1\n",
            "TF-IDF shape (train): (25000, 5000)\n",
            "TF-IDF shape (test): (25000, 5000)\n",
            "\n",
            "=== Logistic Regression ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.87      0.88     12500\n",
            "           1       0.88      0.89      0.88     12500\n",
            "\n",
            "    accuracy                           0.88     25000\n",
            "   macro avg       0.88      0.88      0.88     25000\n",
            "weighted avg       0.88      0.88      0.88     25000\n",
            "\n",
            "Confusion Matrix:\n",
            " [[10931  1569]\n",
            " [ 1377 11123]]\n",
            "\n",
            "=== Naive Bayes ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85     12500\n",
            "           1       0.85      0.85      0.85     12500\n",
            "\n",
            "    accuracy                           0.85     25000\n",
            "   macro avg       0.85      0.85      0.85     25000\n",
            "weighted avg       0.85      0.85      0.85     25000\n",
            "\n",
            "Confusion Matrix:\n",
            " [[10582  1918]\n",
            " [ 1838 10662]]\n",
            "\n",
            "=== SVM ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.86      0.87     12500\n",
            "           1       0.87      0.87      0.87     12500\n",
            "\n",
            "    accuracy                           0.87     25000\n",
            "   macro avg       0.87      0.87      0.87     25000\n",
            "weighted avg       0.87      0.87      0.87     25000\n",
            "\n",
            "Confusion Matrix:\n",
            " [[10805  1695]\n",
            " [ 1615 10885]]\n",
            "\n",
            "Model Performance Summary:\n",
            "Logistic Regression: Acc=0.882, Prec=0.876, Rec=0.890, F1=0.883\n",
            "Naive Bayes: Acc=0.850, Prec=0.848, Rec=0.853, F1=0.850\n",
            "SVM: Acc=0.868, Prec=0.865, Rec=0.871, F1=0.868\n",
            "\n",
            "Best Pipeline Parameters: {'clf__C': 2, 'vectorizer__max_features': 5000, 'vectorizer__ngram_range': (1, 1)}\n",
            "Best CV Accuracy: 0.8793200588114717\n",
            "\n",
            "Review 1: start please give one miss br br unk unk rest cast rendered terrible performances show flat flat flat br br dont know michael madison could allowed on...\n",
            "Predicted: Negative\n",
            "Actual: Negative\n",
            "\n",
            "Review 2: start film requires lot patience focuses mood character development plot simple many scenes take place set frances unk sandy dennis character apartmen...\n",
            "Predicted: Positive\n",
            "Actual: Positive\n",
            "\n",
            "Review 3: start many animation buffs consider unk unk great forgotten genius one special branch art puppet animation invented almost single unk happened almost ...\n",
            "Predicted: Positive\n",
            "Actual: Positive\n",
            "\n",
            "Review 4: start generally love type movie however time found wanting kick screen since cant complain absolutely idiotic things happen dead kids cool alive peopl...\n",
            "Predicted: Positive\n",
            "Actual: Negative\n",
            "\n",
            "Review 5: start like people wrote im die hard mario fan loved game br br game starts slightly boring trust worth soon start hooked levels fun unk hook unk mind ...\n",
            "Predicted: Positive\n",
            "Actual: Positive\n",
            "\n",
            "Final Test Accuracy: 0.87696\n"
          ]
        }
      ]
    }
  ]
}